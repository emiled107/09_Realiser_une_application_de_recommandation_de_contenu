{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Content - Recommender System\n",
    "\n",
    "## Téléchargement et Prétraitement des Données\n",
    "\n",
    "Dans ce notebook, nous allons télécharger les fichiers de données et les prétraiter pour le système de recommandation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import joblib\n",
    "\n",
    "# Importation de TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importation de MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Importation des bibliothèques de scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b872d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement des fichiers de données\n",
    "\n",
    "Nous allons télécharger les fichiers de données depuis les liens fournis et les enregistrer dans le répertoire `../data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour télécharger un fichier\n",
    "def download_file(url, dest_path):\n",
    "    if not os.path.exists(dest_path):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Vérifie si la requête a réussi\n",
    "            with open(dest_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f'Téléchargement terminé : {dest_path}')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Erreur lors du téléchargement : {e}')\n",
    "    else:\n",
    "        print(f'Le fichier {dest_path} existe déjà. Téléchargement non nécessaire.')\n",
    "\n",
    "# URL du fichier zip contenant les données\n",
    "data_url = 'https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+9+-+R%C3%A9alisez+une+application+mobile+de+recommandation+de+contenu/news-portal-user-interactions-by-globocom.zip'\n",
    "\n",
    "# Chemin de destination pour le fichier zip\n",
    "zip_file_path = '../data/news-portal-user-interactions-by-globocom.zip'\n",
    "\n",
    "# Création du répertoire data s'il n'existe pas\n",
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    print('Répertoire ../data créé.')\n",
    "\n",
    "# Téléchargement du fichier zip\n",
    "download_file(data_url, zip_file_path)\n",
    "\n",
    "# Décompression du fichier zip\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('../data')\n",
    "    print('Extraction terminée.')\n",
    "except zipfile.BadZipFile:\n",
    "    print('Erreur : Le fichier zip est corrompu.')\n",
    "\n",
    "# Chemins des fichiers extraits\n",
    "clicks_zip_path = '../data/clicks.zip'\n",
    "clicks_sample_path = '../data/clicks_sample.csv'\n",
    "articles_metadata_path = '../data/articles_metadata.csv'\n",
    "articles_embeddings_path = '../data/articles_embeddings.pickle'\n",
    "\n",
    "# Vérification des fichiers extraits\n",
    "if all(os.path.exists(path) for path in [clicks_zip_path, clicks_sample_path, articles_metadata_path, articles_embeddings_path]):\n",
    "    print(f'Fichiers extraits:\\n- {clicks_zip_path}\\n- {clicks_sample_path}\\n- {articles_metadata_path}\\n- {articles_embeddings_path}')\n",
    "else:\n",
    "    print('Erreur lors de l\\'extraction des fichiers.')\n",
    "\n",
    "# Décompression de clicks.zip\n",
    "if os.path.exists(clicks_zip_path):\n",
    "    try:\n",
    "        with zipfile.ZipFile(clicks_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall('../data')\n",
    "        print('Extraction de clicks.zip terminée.')\n",
    "    except zipfile.BadZipFile:\n",
    "        print('Erreur : Le fichier clicks.zip est corrompu.')\n",
    "\n",
    "# Chemin des fichiers extraits de clicks.zip\n",
    "clicks_path = '../data/clicks.zip'\n",
    "\n",
    "# Décompression de clicks.zip\n",
    "clicks_extract_dir = '../data/clicks/'\n",
    "if not os.path.exists(clicks_extract_dir):\n",
    "    os.makedirs(clicks_extract_dir, exist_ok=True)\n",
    "    try:\n",
    "        with zipfile.ZipFile(clicks_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(clicks_extract_dir)\n",
    "        print('Extraction de clicks.zip terminée.')\n",
    "    except zipfile.BadZipFile:\n",
    "        print('Erreur : Le fichier clicks.zip est corrompu.')\n",
    "else:\n",
    "    print(f'Le répertoire {clicks_extract_dir} existe déjà. Extraction non nécessaire.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et aperçu des données\n",
    "\n",
    "Nous allons maintenant charger les fichiers CSV et afficher un aperçu des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture des fichiers CSV extraits de clicks.zip\n",
    "clicks_files = [os.path.join(clicks_extract_dir, f) for f in os.listdir(clicks_extract_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Lecture des autres fichiers\n",
    "clicks_sample_df = pd.read_csv(clicks_sample_path)\n",
    "articles_metadata_df = pd.read_csv(articles_metadata_path)\n",
    "\n",
    "# Lecture du fichier pickle\n",
    "with open(articles_embeddings_path, 'rb') as file:\n",
    "    articles_embeddings = pickle.load(file)\n",
    "\n",
    "# Aperçu des autres fichiers\n",
    "print(\"Aperçu du fichier clicks_sample.csv:\")\n",
    "print(clicks_sample_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Informations sur le fichier clicks_sample.csv:\")\n",
    "print(clicks_sample_df.info(), \"\\n\")\n",
    "\n",
    "print(\"Aperçu du fichier articles_metadata.csv:\")\n",
    "print(articles_metadata_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Informations sur le fichier articles_metadata.csv:\")\n",
    "print(articles_metadata_df.info(), \"\\n\")\n",
    "\n",
    "print(\"Aperçu des embeddings d'articles:\")\n",
    "print(f\"Nombre d'articles : {len(articles_embeddings)}\")\n",
    "print(f\"Exemple d'embedding pour le premier article : {articles_embeddings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91a45f5",
   "metadata": {},
   "source": [
    "clicks_sample.csv :\n",
    "Contient des données similaires aux fichiers extraits de clicks.zip.\n",
    "Aperçu des 5 premières lignes et informations sur les colonnes et types de données.\n",
    "articles_metadata.csv :\n",
    "Contient des métadonnées sur les articles, comme le nombre de mots et autres caractéristiques.\n",
    "Aperçu des 5 premières lignes et informations sur les colonnes et types de données.\n",
    "articles_embeddings.pickle :\n",
    "Contient les embeddings des articles (représentations vectorielles).\n",
    "Nombre total d'articles : 364047\n",
    "Exemple d'embedding pour le premier article : [0.08109499, 0.0563579, 0.21711417, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41fc6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des clics par article dans clicks_sample.csv\n",
    "clicks_sample_df['click_article_id'].value_counts().head(10).plot(kind='bar')\n",
    "plt.title('Top 10 des articles les plus cliqués')\n",
    "plt.xlabel('ID de l\\'article')\n",
    "plt.ylabel('Nombre de clics')\n",
    "plt.show()\n",
    "\n",
    "# Répartition des clics par environnement de clic dans clicks_sample.csv\n",
    "clicks_sample_df['click_environment'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Répartition des clics par environnement de clic')\n",
    "plt.ylabel('')\n",
    "plt.show()\n",
    "\n",
    "# Distribution du nombre de mots par article dans articles_metadata.csv\n",
    "articles_metadata_df['words_count'].plot(kind='hist', bins=30)\n",
    "plt.title('Distribution du nombre de mots par article')\n",
    "plt.xlabel('Nombre de mots')\n",
    "plt.ylabel('Nombre d\\'articles')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données\n",
    "Nous allons maintenant prétraiter les données pour le modèle de recommandation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des données\n",
    "analysis_results = {}\n",
    "# Valeurs manquantes\n",
    "analysis_results['clicks_sample_missing'] = clicks_sample_df.isnull().sum()\n",
    "analysis_results['articles_metadata_missing'] = articles_metadata_df.isnull().sum()\n",
    "\n",
    "# Doublons\n",
    "analysis_results['clicks_sample_duplicates'] = clicks_sample_df.duplicated().sum()\n",
    "analysis_results['articles_metadata_duplicates'] = articles_metadata_df.duplicated().sum()\n",
    "\n",
    "analysis_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17c1a0",
   "metadata": {},
   "source": [
    "# Développement du Modèle de Recommandation\n",
    "- Filtrage collaboratif article-article basé sur les clics.\n",
    "- Calcul de la similarité cosinus entre les articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4fe94",
   "metadata": {},
   "source": [
    "preapration des data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66190e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation des données\n",
    "# Encodage des variables catégorielles\n",
    "label_encoder = LabelEncoder()\n",
    "clicks_sample_df['user_id'] = label_encoder.fit_transform(clicks_sample_df['user_id'])\n",
    "clicks_sample_df['article_id'] = label_encoder.fit_transform(clicks_sample_df['article_id'])\n",
    "\n",
    "# Normalisation ou standardisation si nécessaire (exemple ici non inclus)\n",
    "\n",
    "# Features basées sur le texte\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(articles_metadata_df['article_content'])\n",
    "\n",
    "# Construction des matrices utilisateur-article\n",
    "user_article_matrix = clicks_sample_df.pivot(index='user_id', columns='article_id', values='interaction')\n",
    "user_article_matrix.fillna(0, inplace=True)\n",
    "\n",
    "# Sauvegarde des matrices pour une utilisation ultérieure\n",
    "user_article_matrix.to_csv('user_article_matrix.csv')\n",
    "\n",
    "# Aperçu des transformations\n",
    "print(\"Aperçu de clicks_sample_df:\")\n",
    "print(clicks_sample_df.head(), \"\\n\")\n",
    "print(\"Aperçu de articles_metadata_df:\")\n",
    "print(articles_metadata_df.head(), \"\\n\")\n",
    "print(\"Aperçu de la matrice utilisateur-article:\")\n",
    "print(user_article_matrix.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2700ba31",
   "metadata": {},
   "source": [
    "## Fonction de recommandation basée sur la similarité article-article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données pour les modèles de recommandation\n",
    "try:\n",
    "    # Filtrage collaboratif article-article basé sur les clics\n",
    "    article_user_matrix = clicks_sample_df.pivot_table(index='user_id', columns='click_article_id', values='session_id', aggfunc='count').fillna(0)\n",
    "    \n",
    "    # Vérifions la matrice utilisateur-article\n",
    "    print(\"Aperçu de la matrice utilisateur-article:\")\n",
    "    print(article_user_matrix.head())\n",
    "\n",
    "    # Convertir la matrice utilisateur-article en tenseur TensorFlow\n",
    "    article_user_matrix_tensor = tf.convert_to_tensor(article_user_matrix.values, dtype=tf.float32)\n",
    "\n",
    "    # Normaliser la matrice utilisateur-article\n",
    "    normalized_article_user_matrix = tf.nn.l2_normalize(article_user_matrix_tensor, axis=0)\n",
    "\n",
    "    # Calcul de la similarité cosinus entre les articles en utilisant TensorFlow\n",
    "    article_similarity = tf.matmul(tf.transpose(normalized_article_user_matrix), normalized_article_user_matrix).numpy()\n",
    "    \n",
    "    # Vérifions la matrice de similarité\n",
    "    print(\"Aperçu de la matrice de similarité:\")\n",
    "    print(article_similarity[:5, :5])  # Affichage d'un aperçu pour éviter trop de données\n",
    "\n",
    "    # Fonction de recommandation basée sur la similarité article-article\n",
    "    def recommend_articles_article_based(article_id, num_recommendations=5):\n",
    "        article_idx = article_user_matrix.columns.get_loc(article_id)\n",
    "        similar_articles = list(enumerate(article_similarity[article_idx]))\n",
    "        similar_articles = sorted(similar_articles, key=lambda x: x[1], reverse=True)\n",
    "        recommended_articles = [article_user_matrix.columns[i[0]] for i in similar_articles[1:num_recommendations+1]]\n",
    "        return recommended_articles\n",
    "\n",
    "    # Exemple de recommandation d'articles pour un article donné\n",
    "    article_id_example = article_user_matrix.columns[0]\n",
    "    print(f\"Recommandations pour l'article {article_id_example} : {recommend_articles_article_based(article_id_example)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Une erreur s'est produite: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d24b1",
   "metadata": {},
   "source": [
    "## Recommandation basée sur les métadonnées des articles (TF-IDF et similarité cosinus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifiez si TensorFlow utilise le GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Chargement des données\n",
    "articles_metadata_df = pd.read_csv('../data/articles_metadata.csv')\n",
    "\n",
    "# Ajouter des colonnes fictives 'title' et 'content' pour la démonstration\n",
    "articles_metadata_df['title'] = [\"Example Title\"] * len(articles_metadata_df)\n",
    "articles_metadata_df['content'] = [\"Example Content\"] * len(articles_metadata_df)\n",
    "\n",
    "# Création de la matrice TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(articles_metadata_df['title'] + \" \" + articles_metadata_df['content'])\n",
    "\n",
    "# Réduction de dimensions avec TruncatedSVD\n",
    "# Assurez-vous que n_components est inférieur ou égal au nombre de caractéristiques\n",
    "n_components = min(100, tfidf_matrix.shape[1])\n",
    "svd = TruncatedSVD(n_components=n_components)  # Réduire à 100 dimensions ou moins\n",
    "reduced_tfidf_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Convertir la matrice TF-IDF réduite en tenseur TensorFlow\n",
    "reduced_tfidf_matrix_tensor = tf.convert_to_tensor(reduced_tfidf_matrix, dtype=tf.float32)\n",
    "\n",
    "# Normaliser la matrice TF-IDF\n",
    "normalized_tfidf_matrix = tf.nn.l2_normalize(reduced_tfidf_matrix_tensor, axis=1)\n",
    "\n",
    "# Fonction pour calculer la similarité cosinus en utilisant des lots\n",
    "def batch_cosine_similarity(matrix, batch_size=1000):\n",
    "    num_rows = matrix.shape[0]\n",
    "    cosine_sim_matrix = np.zeros((num_rows, num_rows), dtype=np.float32)\n",
    "\n",
    "    for start_idx in range(0, num_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_rows)\n",
    "        batch = matrix[start_idx:end_idx]\n",
    "        similarity = tf.matmul(batch, matrix, transpose_b=True).numpy()\n",
    "        cosine_sim_matrix[start_idx:end_idx] = similarity\n",
    "\n",
    "    return cosine_sim_matrix\n",
    "\n",
    "# Calcul de la similarité cosinus en utilisant des lots\n",
    "cosine_sim = batch_cosine_similarity(normalized_tfidf_matrix, batch_size=1000)\n",
    "\n",
    "# Vérifions la matrice de similarité\n",
    "print(\"Aperçu de la matrice de similarité:\")\n",
    "print(cosine_sim[:5, :5])  # Affichage d'un aperçu pour éviter trop de données\n",
    "\n",
    "# Fonction de recommandation basée sur le contenu\n",
    "def recommend_articles_content_based(article_id, num_recommendations=5):\n",
    "    article_idx = articles_metadata_df[articles_metadata_df['article_id'] == article_id].index[0]\n",
    "    similar_articles = list(enumerate(cosine_sim[article_idx]))\n",
    "    similar_articles = sorted(similar_articles, key=lambda x: x[1], reverse=True)\n",
    "    recommended_articles = [articles_metadata_df['article_id'].iloc[i[0]] for i in similar_articles[1:num_recommendations+1]]\n",
    "    return recommended_articles\n",
    "\n",
    "# Exemple de recommandation d'articles basée sur le contenu pour un article donné\n",
    "article_id_example = articles_metadata_df['article_id'].iloc[0]\n",
    "print(f\"Recommandations basées sur le contenu pour l'article {article_id_example} : {recommend_articles_content_based(article_id_example)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b439784",
   "metadata": {},
   "source": [
    "## Fonction de recommandation basée sur le contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1688b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifiez si TensorFlow utilise le GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Chargement des données\n",
    "articles_metadata_df = pd.read_csv('../data/articles_metadata.csv')\n",
    "\n",
    "# Ajouter des colonnes fictives 'title' et 'content' pour la démonstration\n",
    "articles_metadata_df['title'] = [\"Example Title\"] * len(articles_metadata_df)\n",
    "articles_metadata_df['content'] = [\"Example Content\"] * len(articles_metadata_df)\n",
    "\n",
    "# Création de la matrice TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(articles_metadata_df['title'] + \" \" + articles_metadata_df['content'])\n",
    "\n",
    "# Réduction de dimensions avec TruncatedSVD\n",
    "# Assurez-vous que n_components est inférieur ou égal au nombre de caractéristiques\n",
    "n_components = min(100, tfidf_matrix.shape[1])\n",
    "svd = TruncatedSVD(n_components=n_components)  # Réduire à 100 dimensions ou moins\n",
    "reduced_tfidf_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Convertir la matrice TF-IDF réduite en tenseur TensorFlow\n",
    "reduced_tfidf_matrix_tensor = tf.convert_to_tensor(reduced_tfidf_matrix, dtype=tf.float32)\n",
    "\n",
    "# Normaliser la matrice TF-IDF\n",
    "normalized_tfidf_matrix = tf.nn.l2_normalize(reduced_tfidf_matrix_tensor, axis=1)\n",
    "\n",
    "# Fonction pour calculer la similarité cosinus en utilisant des lots\n",
    "def batch_cosine_similarity(matrix, batch_size=1000):\n",
    "    num_rows = matrix.shape[0]\n",
    "    cosine_sim_matrix = np.zeros((num_rows, num_rows), dtype=np.float32)\n",
    "\n",
    "    for start_idx in range(0, num_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_rows)\n",
    "        batch = matrix[start_idx:end_idx]\n",
    "        similarity = tf.matmul(batch, matrix, transpose_b=True).numpy()\n",
    "        cosine_sim_matrix[start_idx:end_idx] = similarity\n",
    "\n",
    "    return cosine_sim_matrix\n",
    "\n",
    "# Calcul de la similarité cosinus en utilisant des lots\n",
    "cosine_sim = batch_cosine_similarity(normalized_tfidf_matrix, batch_size=1000)\n",
    "\n",
    "# Vérifions la matrice de similarité\n",
    "print(\"Aperçu de la matrice de similarité:\")\n",
    "print(cosine_sim[:5, :5])  # Affichage d'un aperçu pour éviter trop de données\n",
    "\n",
    "# Fonction de recommandation basée sur le contenu avec gestion par lots\n",
    "def recommend_articles_content_based(article_id, num_recommendations=5, batch_size=1000):\n",
    "    article_idx = articles_metadata_df[articles_metadata_df['article_id'] == article_id].index[0]\n",
    "    num_articles = cosine_sim.shape[0]\n",
    "\n",
    "    # Calcul de la similarité par lots\n",
    "    similar_articles = []\n",
    "    for start_idx in range(0, num_articles, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_articles)\n",
    "        batch_similarities = cosine_sim[article_idx, start_idx:end_idx]\n",
    "        batch_indices = range(start_idx, end_idx)\n",
    "        similar_articles.extend(zip(batch_indices, batch_similarities))\n",
    "\n",
    "    similar_articles = sorted(similar_articles, key=lambda x: x[1], reverse=True)\n",
    "    recommended_articles = [articles_metadata_df['article_id'].iloc[i[0]] for i in similar_articles[1:num_recommendations+1]]\n",
    "    return recommended_articles\n",
    "\n",
    "# Exemple de recommandation d'articles basée sur le contenu pour un article donné\n",
    "article_id_example = articles_metadata_df['article_id'].iloc[0]\n",
    "print(f\"Recommandations basées sur le contenu pour l'article {article_id_example} : {recommend_articles_content_based(article_id_example)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c433d",
   "metadata": {},
   "source": [
    "## Recommandation hybride combinant filtrage collaboratif et basé sur le contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifiez si TensorFlow utilise le GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Chargement des données\n",
    "articles_metadata_df = pd.read_csv('../data/articles_metadata.csv')\n",
    "clicks_sample_df = pd.read_csv('../data/clicks_sample.csv')\n",
    "\n",
    "# Préparation des données pour le filtrage collaboratif\n",
    "article_user_matrix = clicks_sample_df.pivot_table(index='user_id', columns='click_article_id', values='session_id', aggfunc='count').fillna(0)\n",
    "\n",
    "# Calcul de la similarité cosinus pour le filtrage collaboratif\n",
    "article_similarity = cosine_similarity(article_user_matrix.T)\n",
    "\n",
    "# Fonction de recommandation basée sur le filtrage collaboratif\n",
    "def recommend_articles_article_based(article_id, num_recommendations=5):\n",
    "    article_idx = article_user_matrix.columns.get_loc(article_id)\n",
    "    similar_articles = list(enumerate(article_similarity[article_idx]))\n",
    "    similar_articles = sorted(similar_articles, key=lambda x: x[1], reverse=True)\n",
    "    recommended_articles = [article_user_matrix.columns[i[0]] for i in similar_articles[1:num_recommendations+1]]\n",
    "    return recommended_articles\n",
    "\n",
    "# Ajouter des colonnes fictives 'title' et 'content' pour la démonstration\n",
    "articles_metadata_df['title'] = [\"Example Title\"] * len(articles_metadata_df)\n",
    "articles_metadata_df['content'] = [\"Example Content\"] * len(articles_metadata_df)\n",
    "\n",
    "# Création de la matrice TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(articles_metadata_df['title'] + \" \" + articles_metadata_df['content'])\n",
    "\n",
    "# Réduction de dimensions avec TruncatedSVD\n",
    "n_components = min(100, tfidf_matrix.shape[1])\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "reduced_tfidf_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Convertir la matrice TF-IDF réduite en tenseur TensorFlow\n",
    "reduced_tfidf_matrix_tensor = tf.convert_to_tensor(reduced_tfidf_matrix, dtype=tf.float32)\n",
    "\n",
    "# Normaliser la matrice TF-IDF\n",
    "normalized_tfidf_matrix = tf.nn.l2_normalize(reduced_tfidf_matrix_tensor, axis=1)\n",
    "\n",
    "# Fonction pour calculer la similarité cosinus en utilisant des lots\n",
    "def batch_cosine_similarity(matrix, batch_size=1000):\n",
    "    num_rows = matrix.shape[0]\n",
    "    cosine_sim_matrix = np.zeros((num_rows, num_rows), dtype=np.float32)\n",
    "\n",
    "    for start_idx in range(0, num_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_rows)\n",
    "        batch = matrix[start_idx:end_idx]\n",
    "        similarity = tf.matmul(batch, matrix, transpose_b=True).numpy()\n",
    "        cosine_sim_matrix[start_idx:end_idx] = similarity\n",
    "\n",
    "    return cosine_sim_matrix\n",
    "\n",
    "# Calcul de la similarité cosinus en utilisant des lots\n",
    "cosine_sim = batch_cosine_similarity(normalized_tfidf_matrix, batch_size=1000)\n",
    "\n",
    "# Fonction de recommandation basée sur le contenu\n",
    "def recommend_articles_content_based(article_id, num_recommendations=5):\n",
    "    article_idx = articles_metadata_df[articles_metadata_df['article_id'] == article_id].index[0]\n",
    "    similar_articles = list(enumerate(cosine_sim[article_idx]))\n",
    "    similar_articles = sorted(similar_articles, key=lambda x: x[1], reverse=True)\n",
    "    recommended_articles = [articles_metadata_df['article_id'].iloc[i[0]] for i in similar_articles[1:num_recommendations+1]]\n",
    "    return recommended_articles\n",
    "\n",
    "# Recommandation hybride combinant filtrage collaboratif et basé sur le contenu\n",
    "def hybrid_recommendation(user_id, num_recommendations=5):\n",
    "    # Recommandations basées sur le filtrage collaboratif\n",
    "    user_clicks = article_user_matrix.loc[user_id]\n",
    "    user_clicks = user_clicks[user_clicks > 0]\n",
    "    collaborative_recommendations = []\n",
    "    for article_id in user_clicks.index:\n",
    "        collaborative_recommendations.extend(recommend_articles_article_based(article_id, num_recommendations))\n",
    "    collaborative_recommendations = list(set(collaborative_recommendations))\n",
    "    \n",
    "    # Recommandations basées sur le contenu\n",
    "    content_based_recommendations = []\n",
    "    for article_id in user_clicks.index:\n",
    "        content_based_recommendations.extend(recommend_articles_content_based(article_id, num_recommendations))\n",
    "    content_based_recommendations = list(set(content_based_recommendations))\n",
    "    \n",
    "    # Combiner les recommandations\n",
    "    combined_recommendations = list(set(collaborative_recommendations + content_based_recommendations))\n",
    "    return combined_recommendations[:num_recommendations]\n",
    "\n",
    "# Exemple de recommandation hybride pour un utilisateur donné\n",
    "user_id_example = article_user_matrix.index[0]\n",
    "print(f\"Recommandations hybrides pour l'utilisateur {user_id_example} : {hybrid_recommendation(user_id_example)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde des données prétraitées\n",
    "\n",
    "Nous allons sauvegarder les données prétraitées pour une utilisation ultérieure dans l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Chemins relatifs pour sauvegarder les modèles et les données\n",
    "article_user_matrix_path = 'models/article_user_matrix.pkl'\n",
    "article_similarity_path = 'models/article_similarity.pkl'\n",
    "tfidf_vectorizer_path = 'models/tfidf_vectorizer.pkl'\n",
    "tfidf_matrix_path = 'models/tfidf_matrix.pkl'\n",
    "cosine_sim_path = 'models/cosine_sim.pkl'\n",
    "\n",
    "# Créer le répertoire 'models' s'il n'existe pas\n",
    "import os\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Enregistrer et suivre les modèles\n",
    "joblib.dump(article_user_matrix, article_user_matrix_path)\n",
    "joblib.dump(article_similarity, article_similarity_path)\n",
    "joblib.dump(tfidf_vectorizer, tfidf_vectorizer_path)\n",
    "joblib.dump(tfidf_matrix, tfidf_matrix_path)\n",
    "joblib.dump(cosine_sim, cosine_sim_path)\n",
    "\n",
    "print(\"Modèles et données préparées sauvegardés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd2b14",
   "metadata": {},
   "source": [
    "## Déploiement Serverless avec Azure Functions\n",
    "Api local de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b82997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Chemins relatifs des fichiers sauvegardés\n",
    "article_user_matrix_path = 'models/article_user_matrix.pkl'\n",
    "article_similarity_path = 'models/article_similarity.pkl'\n",
    "tfidf_vectorizer_path = 'models/tfidf_vectorizer.pkl'\n",
    "tfidf_matrix_path = 'models/tfidf_matrix.pkl'\n",
    "cosine_sim_path = 'models/cosine_sim.pkl'\n",
    "\n",
    "# Charger les fichiers sauvegardés\n",
    "article_user_matrix = joblib.load(article_user_matrix_path)\n",
    "article_similarity = joblib.load(article_similarity_path)\n",
    "tfidf_vectorizer = joblib.load(tfidf_vectorizer_path)\n",
    "tfidf_matrix = joblib.load(tfidf_matrix_path)\n",
    "cosine_sim = joblib.load(cosine_sim_path)\n",
    "\n",
    "# Fonction de recommandation collaborative\n",
    "def collaborative_recommendation(user_id, num_recommendations=5):\n",
    "    if user_id in article_user_matrix.index:\n",
    "        user_clicks = article_user_matrix.loc[user_id]\n",
    "        user_clicks = user_clicks[user_clicks > 0]\n",
    "        collaborative_recommendations = []\n",
    "        for article_id in user_clicks.index:\n",
    "            similar_articles = np.argsort(article_similarity[article_id])[::-1]\n",
    "            collaborative_recommendations.extend(similar_articles[:num_recommendations])\n",
    "        collaborative_recommendations = list(set(collaborative_recommendations))\n",
    "        return collaborative_recommendations[:num_recommendations]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Fonction de recommandation basée sur le contenu\n",
    "def content_based_recommendation(user_id, num_recommendations=5):\n",
    "    if user_id in article_user_matrix.index:\n",
    "        user_clicks = article_user_matrix.loc[user_id]\n",
    "        user_clicks = user_clicks[user_clicks > 0]\n",
    "        content_based_recommendations = []\n",
    "        for article_id in user_clicks.index:\n",
    "            similar_articles = np.argsort(cosine_sim[article_id])[::-1]\n",
    "            content_based_recommendations.extend(similar_articles[:num_recommendations])\n",
    "        content_based_recommendations = list(set(content_based_recommendations))\n",
    "        return content_based_recommendations[:num_recommendations]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Fonction de recommandation hybride\n",
    "def hybrid_recommendation(user_id, num_recommendations=5):\n",
    "    collaborative_recommendations = collaborative_recommendation(user_id, num_recommendations)\n",
    "    content_based_recommendations = content_based_recommendation(user_id, num_recommendations)\n",
    "    combined_recommendations = list(set(collaborative_recommendations + content_based_recommendations))\n",
    "    return combined_recommendations[:num_recommendations]\n",
    "\n",
    "@app.route('/recommend/collaborative', methods=['GET'])\n",
    "def recommend_collaborative():\n",
    "    user_id = int(request.args.get('user_id'))\n",
    "    recommendations = collaborative_recommendation(user_id)\n",
    "    return jsonify(recommendations)\n",
    "\n",
    "@app.route('/recommend/content', methods=['GET'])\n",
    "def recommend_content():\n",
    "    user_id = int(request.args.get('user_id'))\n",
    "    recommendations = content_based_recommendation(user_id)\n",
    "    return jsonify(recommendations)\n",
    "\n",
    "@app.route('/recommend/hybrid', methods=['GET'])\n",
    "def recommend_hybrid():\n",
    "    user_id = int(request.args.get('user_id'))\n",
    "    recommendations = hybrid_recommendation(user_id)\n",
    "    return jsonify(recommendations)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5d706",
   "metadata": {},
   "source": [
    "## Web app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbdb44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__, template_folder='../app/templates')\n",
    "\n",
    "# Chemins relatifs des fichiers sauvegardés\n",
    "article_user_matrix_path = 'models/article_user_matrix.pkl'\n",
    "article_similarity_path = 'models/article_similarity.pkl'\n",
    "tfidf_vectorizer_path = 'models/tfidf_vectorizer.pkl'\n",
    "tfidf_matrix_path = 'models/tfidf_matrix.pkl'\n",
    "cosine_sim_path = 'models/cosine_sim.pkl'\n",
    "\n",
    "# Charger les fichiers sauvegardés\n",
    "article_user_matrix = joblib.load(article_user_matrix_path)\n",
    "article_similarity = joblib.load(article_similarity_path)\n",
    "tfidf_vectorizer = joblib.load(tfidf_vectorizer_path)\n",
    "tfidf_matrix = joblib.load(tfidf_matrix_path)\n",
    "cosine_sim = joblib.load(cosine_sim_path)\n",
    "\n",
    "# Fonction de recommandation collaborative\n",
    "def collaborative_recommendation(user_id, num_recommendations=5):\n",
    "    if user_id in article_user_matrix.index:\n",
    "        user_clicks = article_user_matrix.loc[user_id]\n",
    "        user_clicks = user_clicks[user_clicks > 0]\n",
    "        collaborative_recommendations = []\n",
    "        for article_id in user_clicks.index:\n",
    "            similar_articles = np.argsort(article_similarity[article_id])[::-1]\n",
    "            collaborative_recommendations.extend(similar_articles[:num_recommendations])\n",
    "        collaborative_recommendations = list(set(collaborative_recommendations))\n",
    "        return collaborative_recommendations[:num_recommendations]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Fonction de recommandation basée sur le contenu\n",
    "def content_based_recommendation(user_id, num_recommendations=5):\n",
    "    if user_id in article_user_matrix.index:\n",
    "        user_clicks = article_user_matrix.loc[user_id]\n",
    "        user_clicks = user_clicks[user_clicks > 0]\n",
    "        content_based_recommendations = []\n",
    "        for article_id in user_clicks.index:\n",
    "            similar_articles = np.argsort(cosine_sim[article_id])[::-1]\n",
    "            content_based_recommendations.extend(similar_articles[:num_recommendations])\n",
    "        content_based_recommendations = list(set(content_based_recommendations))\n",
    "        return content_based_recommendations[:num_recommendations]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Fonction de recommandation hybride\n",
    "def hybrid_recommendation(user_id, num_recommendations=5):\n",
    "    collaborative_recommendations = collaborative_recommendation(user_id, num_recommendations)\n",
    "    content_based_recommendations = content_based_recommendation(user_id, num_recommendations)\n",
    "    combined_recommendations = list(set(collaborative_recommendations + content_based_recommendations))\n",
    "    return combined_recommendations[:num_recommendations]\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/recommend/collaborative', methods=['GET'])\n",
    "def recommend_collaborative():\n",
    "    user_id = int(request.args.get('user_id'))\n",
    "    recommendations = collaborative_recommendation(user_id)\n",
    "    return jsonify(recommendations)\n",
    "\n",
    "@app.route('/recommend/content', methods=['GET'])\n",
    "def recommend_content():\n",
    "    user_id = int(request.args.get('user_id'))\n",
    "    recommendations = content_based_recommendation(user_id)\n",
    "    return jsonify(recommendations)\n",
    "\n",
    "@app.route('/recommend/hybrid', methods=['GET'])\n",
    "def recommend_hybrid():\n",
    "    user_id = int(request.args.get('user_id'))\n",
    "    recommendations = hybrid_recommendation(user_id)\n",
    "    return jsonify(recommendations)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
